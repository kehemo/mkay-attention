#include <cuda_bf16.h>
#include <algorithm>
#include <chrono>
#include <cmath>
#include <cstdint>
#include <cstdio>
#include <cuda_runtime.h>
#include <fstream>
#include <functional>
#include <iostream>
#include <random>
#include <string>
#include <vector>
#include <cassert>
#pragma once

void cuda_check(cudaError_t code, const char *file, int line)
{
    if (code != cudaSuccess)
    {
        std::cerr << "CUDA error at " << file << ":" << line << ": "
                  << cudaGetErrorString(code) << std::endl;
        exit(1);
    }
}

#define CUDA_CHECK(x)                        \
    do                                       \
    {                                        \
        cuda_check((x), __FILE__, __LINE__); \
    } while (0)
typedef __nv_bfloat16 num;
typedef __nv_bfloat162 num_reg;

__device__ __forceinline__ void cp_async4(void *smem_ptr, const void *glob_ptr)
{
    const int BYTES = 16;
    uint32_t smem = static_cast<uint32_t>(__cvta_generic_to_shared(smem_ptr));
    asm volatile(
        "cp.async.cg.shared.global [%0], [%1], %2;" ::"r"(smem),
        "l"(glob_ptr),
        "n"(BYTES));
}

__device__ __forceinline__ void async_memcpy_waitall()
{
    asm volatile("cp.async.wait_all;\n" ::);
}

__global__ void add_one_kernel(float *data, int size)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < size)
    {
        data[idx] += 1.0;
    }
}

////////////////////////////////////////////////////////////////////////////////
///  YOU DO NOT NEED TO MODIFY THE CODE ABOVE HERE (unless you want to).     ///
////////////////////////////////////////////////////////////////////////////////

/// <--- your code here --->

constexpr int tile_dim = 32;
constexpr int num_threads_axis = 32;

__global__ void compute_attn_scores(
    const num *q, const num *k, const num *v,
    num *scores,
    int batch_size, int nheads, int seqlen, int head_dim,
    int64_t batch_stride, int64_t token_stride, int64_t head_stride, int64_t dim_stride)
{
    /*
    INPUTS:
    q,k,v === (batch_size, seqlen, nheads, head_dim)
    OUTPUTS:
    scores === (batch_size, nheads, seqlen, seqlen)
    NOTES:
    Computes q @ k.T along the seqlen dimension
    */

    uint32_t block_id = blockIdx.x;
    uint32_t batch_id = block_id / nheads;
    uint32_t head_id = block_id % nheads;

    uint32_t tile_i = blockIdx.y;
    uint32_t tile_j = blockIdx.z;
    uint32_t thread_i = threadIdx.x;
    uint32_t thread_j = threadIdx.y;
    num softmax_score = hrsqrt(__int2bfloat16_rd(head_dim));

    for (int row = thread_i; row < tile_dim; row += num_threads_axis)
    {
        for (int col = thread_j; col < tile_dim; col += num_threads_axis)
        {
            // compute "score[i][j] = sum_k q[i][d] * k[j][d]"
            num sum = __float2bfloat16(0.0);
            int i = tile_i * tile_dim + row;
            int j = tile_j * tile_dim + col;

            for (int d = 0; d < head_dim; d++)
            {

                uint32_t q_idx = batch_id * batch_stride + i * token_stride + head_id * head_stride + d * dim_stride;
                uint32_t k_idx = batch_id * batch_stride + j * token_stride + head_id * head_stride + d * dim_stride;

                num q_val = q[q_idx];
                num k_val = k[k_idx];

                sum += q_val * k_val;
            }

            uint32_t batch_stride_out = nheads * seqlen * seqlen;
            uint32_t head_stride_out = seqlen * seqlen;
            uint32_t token_stride_out = seqlen;
            uint32_t o_idx = batch_id * batch_stride_out + head_id * head_stride_out + i * token_stride_out + j;
            scores[o_idx] = sum * softmax_score;
        }
    }
}

__global__ void compute_attn_softmax(
    const num *S,
    num *P,
    int batch_size, int nheads, int seqlen, int head_dim)
{
    /*
    INPUTS:
    S === (batch_size, nheads, seqlen, seqlen)
        - attention scores (not softmaxed)
    OUTPUTS:
    P === (batch_size, nheads, seqlen, seqlen)
        - attention scores (softmaxed)
    */

    uint32_t batch_stride_S = nheads * seqlen * seqlen;
    uint32_t head_stride_S = seqlen * seqlen;
    uint32_t token_stride_S = seqlen;

    uint32_t batch_id = blockIdx.x;
    uint32_t head_id = blockIdx.y;
    uint32_t thread_id = threadIdx.x;
    uint32_t num_threads = blockDim.x;

    for (uint32_t query_id = thread_id; query_id < seqlen; query_id += num_threads)
    {
        // UHHH, we need a -inf for BF16, but i dont want to use -INFINITY in case of overflow or something
        num max_el = __float2bfloat16(-100.0);
        for (uint32_t key_id = 0; key_id < seqlen; key_id++)
        {
            uint32_t s_idx = batch_id * batch_stride_S + head_id * head_stride_S + query_id * token_stride_S + key_id;
            max_el = __hmax(max_el, S[s_idx]);
        }

        num sum = __float2bfloat16(0.0);
        for (uint32_t key_id = 0; key_id < seqlen; key_id++)
        {
            uint32_t s_idx = batch_id * batch_stride_S + head_id * head_stride_S + query_id * token_stride_S + key_id;
            P[s_idx] = hexp2(S[s_idx] - max_el);
            sum += P[s_idx];
        }

        for (uint32_t key_id = 0; key_id < seqlen; key_id++)
        {
            uint32_t s_idx = batch_id * batch_stride_S + head_id * head_stride_S + query_id * token_stride_S + key_id;
            P[s_idx] /= sum;
        }
    }
}

constexpr uint32_t n_threads_seqlen_k3 = 32;
constexpr uint32_t n_threads_head_dim_k3 = 4;
constexpr uint32_t seqlen_tile_k3 = n_threads_seqlen_k3;
constexpr uint32_t head_dim_tile_k3 = n_threads_head_dim_k3 * 4;
__global__ void compute_attn_output(
    const num *P, const num *V,
    num *O,
    int batch_size, int nheads, int seqlen, int head_dim,
    int64_t batch_stride_qkv, int64_t token_stride_qkv, int64_t head_stride_qkv, int64_t dim_stride_qkv)
{
    /*
    INPUTS:
    P === (batch_size, nheads, seqlen, seqlen)
        - attention scores (softmaxed)
    V === (batch_size, seqlen, nheads, head_dim)
        - values
    OUTPUTS:
    O === (batch_size, seqlen, nheads, head_dim)
        - attention output
    */

    uint32_t block_id = blockIdx.x;
    uint32_t batch_id = block_id / nheads;
    uint32_t head_id = block_id % nheads;

    uint32_t tile_seqlen = blockIdx.y;
    uint32_t tile_head_dim = blockIdx.z;
    uint32_t thread_seqlen = threadIdx.x;
    uint32_t thread_head_dim = threadIdx.y;

    for (int row = thread_seqlen; row < seqlen_tile_k3; row += n_threads_seqlen_k3)
    {
        for (int col = thread_head_dim; col < head_dim_tile_k3; col += n_threads_head_dim_k3)
        {
            num sum = __float2bfloat16(0.0);
            int query_id = tile_seqlen * seqlen_tile_k3 + row;
            int head_dim_id = tile_head_dim * head_dim_tile_k3 + col;

            if (head_dim_id >= head_dim)
            {
                break;
            }

            uint32_t batch_stride_P = nheads * seqlen * seqlen;
            uint32_t head_stride_P = seqlen * seqlen;
            uint32_t token_stride_P = seqlen;

            for (int key_id = 0; key_id < seqlen; key_id++)
            {
                uint32_t p_idx = batch_id * batch_stride_P + head_id * head_stride_P + query_id * token_stride_P + key_id;
                uint32_t v_idx = batch_id * batch_stride_qkv + key_id * token_stride_qkv + head_id * head_stride_qkv + head_dim_id * dim_stride_qkv;
                sum += P[p_idx] * V[v_idx];
            }

            uint32_t batch_stride_O = seqlen * nheads * head_dim;
            uint32_t query_stride_O = nheads * head_dim;
            uint32_t head_stride_O = head_dim;
            uint32_t o_idx = batch_id * batch_stride_O + query_id * query_stride_O + head_id * head_stride_O + head_dim_id;
            O[o_idx] = sum;
        }
    }
}

struct attention_params
{
    int batch_size;
    int nheads;
    int seqlen;
    int head_dim;
    int64_t batch_stride;
    int64_t token_stride;
    int64_t head_stride;
    int64_t dim_stride;
    float softmax_scale;
};

__device__ num_reg load_reg(num *addr)
{
    return *reinterpret_cast<num_reg *>(addr);
}

constexpr int nums_in_vec = sizeof(float4) / sizeof(num);
constexpr int shmem_max_size = 100000;
constexpr int d_padding = nums_in_vec;
constexpr int b_c_padding = 2;

template <int B_r, int B_c_split_size, int d_split_size, int splits>
struct shmem_t
{
    num K_tile[B_c_split_size * splits * (d_split_size * splits)];
    num V_tile[B_c_split_size * splits * (d_split_size * splits)];
    num Q_tile[B_r * (d_split_size * splits + d_padding)];
    num S_tile[B_r * (B_c_split_size * splits + b_c_padding)];
    float exp_diffs[B_r];
    float ls[B_r];
};

constexpr __host__ __device__ int ceil_div(int a, int b)
{
    return (a + b - 1) / b;
}

constexpr int max_threads = 128;
constexpr int threads_per_warp = 32;
constexpr int tc_m_size = 16;
constexpr int tc_n_size = 8;
constexpr int tc_k_size = 16;
constexpr int tc_A_size = 4;
constexpr int tc_B_size = 2;
constexpr int tc_CD_size = 4;
constexpr __device__ int tc_A_m_stride[tc_A_size] = {0, 8, 0, 8};
constexpr __device__ int tc_A_k_stride[tc_A_size] = {0, 0, 8, 8};
constexpr __device__ int tc_B_k_stride[tc_B_size] = {0, 8};
constexpr __device__ int tc_B_n_stride[tc_B_size] = {0, 0};
constexpr __device__ int tc_CD_m_stride[tc_CD_size] = {0, 0, 8, 8};
constexpr __device__ int tc_CD_n_stride[tc_CD_size] = {0, 1, 0, 1};

template <int B_r, int B_c_split_size, int d_split_size, int splits>
__global__ void __launch_bounds__(max_threads) flash_attention(
    const num *__restrict__ q, const num *__restrict__ k, const num *__restrict__ v, num *__restrict__ o, num *__restrict__ L, attention_params params)
{
    extern __shared__ shmem_t<B_r, B_c_split_size, d_split_size, splits> shmem[];
    int batch_idx = blockIdx.x;
    int head_idx = blockIdx.y;
    int i = blockIdx.z;
    int b_r_idx = threadIdx.x;
    int split_idx = threadIdx.y;
    int thread_idx = b_r_idx + split_idx * B_r;
    int warp_thread_idx = thread_idx % threads_per_warp;
    int warp_idx = thread_idx / threads_per_warp;
    constexpr int threads = B_r * splits;
    constexpr int warps = threads / threads_per_warp;

    int qkvo_offset = batch_idx * params.batch_stride + head_idx * params.head_stride;
    const num *cur_q = &q[qkvo_offset];
    const num *cur_k = &k[qkvo_offset];
    const num *cur_v = &v[qkvo_offset];
    num *cur_o = &o[qkvo_offset];

    int L_offset = (batch_idx * params.nheads + head_idx) * params.seqlen;
    num *cur_L = &L[L_offset];

    int qo_tile_offset = i * B_r * params.token_stride;
    const num *tile_q = &cur_q[qo_tile_offset];
    num *tile_o = &cur_o[qo_tile_offset];
    int L_tile_offset = i * B_r;
    num *tile_L = &cur_L[L_tile_offset];

    float l = 0;
    num m = __float2bfloat16(-INFINITY);

    int b_r_limit = min(B_r, params.seqlen - i * B_r);
    int d_split_start = split_idx * d_split_size;
    int d_split_end = min((split_idx + 1) * d_split_size, params.head_dim);
    int b_c_split_start = split_idx * B_c_split_size;
    int b_c_split_end = min((split_idx + 1) * B_c_split_size, params.seqlen);
    constexpr int B_c = B_c_split_size * splits;
    constexpr int padded_B_c = B_c + b_c_padding;
    constexpr int max_d = d_split_size * splits;
    constexpr int padded_d = max_d + d_padding;

    if (params.dim_stride == 1 && max_d % nums_in_vec == 0 && params.head_dim == max_d && (i + 1) * B_r <= params.seqlen)
    {
        for (int block_idx = thread_idx * nums_in_vec; block_idx < B_r * max_d; block_idx += nums_in_vec * threads)
        {
            int b_r_idx = block_idx / max_d;
            int d_idx = block_idx % max_d;
            int q_idx = b_r_idx * params.token_stride + d_idx * params.dim_stride;
            cp_async4(&shmem->Q_tile[b_r_idx * padded_d + d_idx], &tile_q[q_idx]);
        }
    }
    else
    {
        for (int d_idx = d_split_start; d_idx < d_split_end; d_idx++)
        {
            int q_idx = b_r_idx * params.token_stride + d_idx * params.dim_stride;
            if (b_r_idx < b_r_limit)
            {
                shmem->Q_tile[b_r_idx * padded_d + d_idx] = tile_q[q_idx];
            }
        }
    }

    int tc_A_m_offset = warp_thread_idx / 4;
    int tc_A_k_offset = (warp_thread_idx % 4) * 2;
    int tc_B_k_offset = (warp_thread_idx % 4) * 2;
    int tc_B_n_offset = warp_thread_idx / 4 + warp_idx * tc_n_size;
    int tc_CD_m_offset = warp_thread_idx / 4;
    int tc_CD_n_offset = (warp_thread_idx % 4) * 2 + warp_idx * tc_n_size;

    constexpr int tc_n_warp_size = tc_n_size * warps;
    constexpr int tc_S_m = ceil_div(B_r, tc_m_size);
    constexpr int tc_S_n = ceil_div(B_c, tc_n_warp_size);
    constexpr int tc_S_k = ceil_div(max_d, tc_k_size);
    constexpr int tc_O_m = ceil_div(B_r, tc_m_size);
    constexpr int tc_O_n = ceil_div(max_d, tc_n_warp_size);
    constexpr int tc_O_k = ceil_div(B_c, tc_k_size);
    uint tc_O[tc_CD_size][tc_O_m][tc_O_n] = {0};

    int T_c = ceil_div(params.seqlen, B_c);
    for (int j = 0; j < T_c; j++)
    {
        int b_c_limit = min(B_c, params.seqlen - j * B_c);
        int kv_tile_offset = j * B_c * params.token_stride;
        const num *tile_k = &cur_k[kv_tile_offset];
        const num *tile_v = &cur_v[kv_tile_offset];
        if (params.dim_stride == 1 && max_d % nums_in_vec == 0 && b_c_limit == B_c && params.head_dim == max_d)
        {
            for (int block_idx = thread_idx * nums_in_vec; block_idx < B_c * max_d; block_idx += threads * nums_in_vec)
            {
                int b_c_idx = block_idx / max_d;
                int d_idx = block_idx % max_d;
                int kv_idx = b_c_idx * params.token_stride + d_idx * params.dim_stride;
                cp_async4(&shmem->K_tile[b_c_idx * max_d + d_idx], &tile_k[kv_idx]);
                cp_async4(&shmem->V_tile[b_c_idx * max_d + d_idx], &tile_v[kv_idx]);
            }
        }
        else
        {
            for (int block_idx = thread_idx; block_idx < B_c * params.head_dim; block_idx += threads)
            {
                int d_idx = block_idx / B_c;
                int b_c_idx = block_idx % B_c;
                int kv_idx = b_c_idx * params.token_stride + d_idx * params.dim_stride;
                if (b_c_idx < b_c_limit)
                {
                    shmem->K_tile[b_c_idx * max_d + d_idx] = tile_k[kv_idx];
                    shmem->V_tile[b_c_idx * max_d + d_idx] = tile_v[kv_idx];
                }
            }
        }
        async_memcpy_waitall();
        __syncthreads();

        uint tc_Q[tc_A_size][tc_S_k];
        uint tc_K[tc_B_size][tc_S_k][tc_S_n];
        for (int tc_k_idx = 0; tc_k_idx < tc_S_k; tc_k_idx++)
        {
            int d_base = tc_k_size * tc_k_idx;
            for (int tc_n_idx = 0; tc_n_idx < tc_S_n; tc_n_idx++)
            {
                int b_c_base = tc_n_idx * tc_n_warp_size;
                for (int tc_B_idx = 0; tc_B_idx < tc_B_size; tc_B_idx++)
                {
                    int d_idx = d_base + tc_B_k_offset + tc_B_k_stride[tc_B_idx];
                    int b_c_idx = b_c_base + tc_B_n_offset + tc_B_n_stride[tc_B_idx];
                    num_reg reg = (d_idx + 1 < params.head_dim && b_c_idx < b_c_limit) ? load_reg(&shmem->K_tile[b_c_idx * max_d + d_idx]) : __halves2bfloat162((d_idx < params.head_dim && b_c_idx < b_c_limit) ? shmem->K_tile[b_c_idx * max_d + d_idx] : __float2bfloat16(0.0f), __float2bfloat16(0.0f));
                    tc_K[tc_B_idx][tc_k_idx][tc_n_idx] = __BFLOAT162_TO_UI(reg);
                }
            }
        }
        for (int tc_m_idx = 0; tc_m_idx < tc_S_m; tc_m_idx++)
        {
            int b_r_base = tc_m_idx * tc_m_size;
            for (int tc_k_idx = 0; tc_k_idx < tc_S_k; tc_k_idx++)
            {
                int d_base = tc_k_size * tc_k_idx;
                for (int tc_A_idx = 0; tc_A_idx < tc_A_size; tc_A_idx++)
                {
                    int d_idx = d_base + tc_A_k_offset + tc_A_k_stride[tc_A_idx];
                    int b_r_idx = b_r_base + tc_A_m_offset + tc_A_m_stride[tc_A_idx];
                    num_reg reg = (d_idx + 1 < params.head_dim && b_r_idx < b_r_limit) ? load_reg(&shmem->Q_tile[b_r_idx * padded_d + d_idx]) : __halves2bfloat162((d_idx < params.head_dim && b_r_idx < b_r_limit) ? shmem->Q_tile[b_r_idx * padded_d + d_idx] : __float2bfloat16(0.0f), __float2bfloat16(0.0f));
                    tc_Q[tc_A_idx][tc_k_idx] = __BFLOAT162_TO_UI(reg);
                }
            }
            for (int tc_n_idx = 0; tc_n_idx < tc_S_n; tc_n_idx++)
            {
                uint tc_S[tc_CD_size] = {0};
                int b_c_base = tc_n_idx * tc_n_warp_size;
                for (int tc_k_idx = 0; tc_k_idx < tc_S_k; tc_k_idx++)
                {
                    asm("mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32"
                        "{%0, %1, %2, %3},     /* 'D' matrix */"
                        "{%4, %5, %6, %7},     /* 'A' matrix */"
                        "{%8, %9},             /* 'B' matrix */"
                        "{%10, %11, %12, %13}; /* 'C' matrix */"
                        : "+r"(tc_S[0]),
                          "+r"(tc_S[1]),
                          "+r"(tc_S[2]),
                          "+r"(tc_S[3])
                        : "r"(tc_Q[0][tc_k_idx]),
                          "r"(tc_Q[1][tc_k_idx]),
                          "r"(tc_Q[2][tc_k_idx]),
                          "r"(tc_Q[3][tc_k_idx]),
                          "r"(tc_K[0][tc_k_idx][tc_n_idx]),
                          "r"(tc_K[1][tc_k_idx][tc_n_idx]),
                          "r"(tc_S[0]),
                          "r"(tc_S[1]),
                          "r"(tc_S[2]),
                          "r"(tc_S[3]));
                }
                for (int tc_CD_idx = 0; tc_CD_idx < tc_CD_size; tc_CD_idx++)
                {
                    int b_r_idx = b_r_base + tc_CD_m_offset + tc_CD_m_stride[tc_CD_idx];
                    int b_c_idx = b_c_base + tc_CD_n_offset + tc_CD_n_stride[tc_CD_idx];
                    if (b_r_idx < b_r_limit && b_c_idx < b_c_limit)
                    {
                        shmem->S_tile[b_r_idx * padded_B_c + b_c_idx] = __float2bfloat16(__uint_as_float(tc_S[tc_CD_idx]) * params.softmax_scale);
                    }
                }
            }
        }
        __syncthreads();
        num S_tile_row[B_c];
        num new_m = m;
        for (int b_c_idx = 0; b_c_idx < B_c; b_c_idx++)
        {
            num s_val = shmem->S_tile[b_r_idx * padded_B_c + b_c_idx];
            new_m = __hmax(new_m, s_val);
            S_tile_row[b_c_idx] = s_val;
        }
        float exp_diff = exp2f(__bfloat162float(m - new_m));
        float new_l = exp_diff * l;
        for (int b_c_idx = 0; b_c_idx < B_c; b_c_idx++)
        {
            S_tile_row[b_c_idx] = hexp2(S_tile_row[b_c_idx] - __float2bfloat16(new_m));
            new_l += b_c_idx < b_c_limit ? __bfloat162float(S_tile_row[b_c_idx]) : 0.0f;
        }
        for (int b_c_idx = 0; b_c_idx < B_c; b_c_idx++)
        {
            if (b_c_idx >= b_c_split_start && b_c_idx < b_c_split_end)
            {
                shmem->S_tile[b_r_idx * padded_B_c + b_c_idx] = S_tile_row[b_c_idx];
            }
        }
        shmem->exp_diffs[b_r_idx] = exp_diff;
        __syncthreads();
        uint tc_P[tc_A_size][tc_O_k];
        uint tc_V[tc_B_size][tc_O_k][tc_O_n];
        for (int tc_k_idx = 0; tc_k_idx < tc_O_k; tc_k_idx++)
        {
            int b_c_base = tc_k_idx * tc_k_size;
            for (int tc_n_idx = 0; tc_n_idx < tc_O_n; tc_n_idx++)
            {
                int d_base = tc_n_idx * tc_n_warp_size;
                for (int tc_B_idx = 0; tc_B_idx < tc_B_size; tc_B_idx++)
                {
                    int b_c_idx = b_c_base + tc_B_k_offset + tc_B_k_stride[tc_B_idx];
                    int d_idx = d_base + tc_B_n_offset + tc_B_n_stride[tc_B_idx];
                    num_reg reg = __halves2bfloat162((b_c_idx < b_c_limit && d_idx < params.head_dim) ? shmem->V_tile[b_c_idx * max_d + d_idx] : __float2bfloat16(0.0f), (b_c_idx + 1 < b_c_limit && d_idx < params.head_dim) ? shmem->V_tile[(b_c_idx + 1) * max_d + d_idx] : __float2bfloat16(0.0f));
                    tc_V[tc_B_idx][tc_k_idx][tc_n_idx] = __BFLOAT162_TO_UI(reg);
                }
            }
        }
        for (int tc_m_idx = 0; tc_m_idx < tc_O_m; tc_m_idx++)
        {
            int b_r_base = tc_m_idx * tc_m_size;
            for (int tc_k_idx = 0; tc_k_idx < tc_O_k; tc_k_idx++)
            {
                int b_c_base = tc_k_idx * tc_k_size;
                for (int tc_A_idx = 0; tc_A_idx < tc_A_size; tc_A_idx++)
                {
                    int b_c_idx = b_c_base + tc_A_k_offset + tc_A_k_stride[tc_A_idx];
                    int b_r_idx = b_r_base + tc_A_m_offset + tc_A_m_stride[tc_A_idx];
                    num_reg reg = (b_c_idx + 1 < b_c_limit && b_r_idx < b_r_limit) ? load_reg(&shmem->S_tile[b_r_idx * padded_B_c + b_c_idx]) : __halves2bfloat162((b_c_idx < b_c_limit && b_r_idx < b_r_limit) ? shmem->S_tile[b_r_idx * padded_B_c + b_c_idx] : __float2bfloat16(0.0f), __float2bfloat16(0.0f));
                    tc_P[tc_A_idx][tc_k_idx] = __BFLOAT162_TO_UI(reg);
                }
            }
            for (int tc_n_idx = 0; tc_n_idx < tc_O_n; tc_n_idx++)
            {
                for (int tc_CD_idx = 0; tc_CD_idx < tc_CD_size; tc_CD_idx++)
                {
                    int b_r_idx = b_r_base + tc_CD_m_offset + tc_CD_m_stride[tc_CD_idx];
                    tc_O[tc_CD_idx][tc_m_idx][tc_n_idx] = __float_as_uint(__uint_as_float(tc_O[tc_CD_idx][tc_m_idx][tc_n_idx]) * shmem->exp_diffs[b_r_idx]);
                }
                for (int tc_k_idx = 0; tc_k_idx < tc_O_k; tc_k_idx++)
                {
                    asm("mma.sync.aligned.m16n8k16.row.col.f32.bf16.bf16.f32"
                        "{%0, %1, %2, %3},     /* 'D' matrix */"
                        "{%4, %5, %6, %7},     /* 'A' matrix */"
                        "{%8, %9},             /* 'B' matrix */"
                        "{%10, %11, %12, %13}; /* 'C' matrix */"
                        : "+r"(tc_O[0][tc_m_idx][tc_n_idx]),
                          "+r"(tc_O[1][tc_m_idx][tc_n_idx]),
                          "+r"(tc_O[2][tc_m_idx][tc_n_idx]),
                          "+r"(tc_O[3][tc_m_idx][tc_n_idx])
                        : "r"(tc_P[0][tc_k_idx]),
                          "r"(tc_P[1][tc_k_idx]),
                          "r"(tc_P[2][tc_k_idx]),
                          "r"(tc_P[3][tc_k_idx]),
                          "r"(tc_V[0][tc_k_idx][tc_n_idx]),
                          "r"(tc_V[1][tc_k_idx][tc_n_idx]),
                          "r"(tc_O[0][tc_m_idx][tc_n_idx]),
                          "r"(tc_O[1][tc_m_idx][tc_n_idx]),
                          "r"(tc_O[2][tc_m_idx][tc_n_idx]),
                          "r"(tc_O[3][tc_m_idx][tc_n_idx]));
                }
            }
        }
        m = new_m;
        l = new_l;
    }
    shmem->ls[b_r_idx] = l;
    __syncthreads();
    for (int tc_m_idx = 0; tc_m_idx < tc_O_m; tc_m_idx++)
    {
        int b_r_base = tc_m_idx * tc_m_size;
        for (int tc_n_idx = 0; tc_n_idx < tc_O_n; tc_n_idx++)
        {
            int d_base = tc_n_idx * tc_n_warp_size;
            for (int tc_CD_idx = 0; tc_CD_idx < tc_CD_size; tc_CD_idx++)
            {
                int b_r_idx = b_r_base + tc_CD_m_offset + tc_CD_m_stride[tc_CD_idx];
                int d_idx = d_base + tc_CD_n_offset + tc_CD_n_stride[tc_CD_idx];
                int o_idx = b_r_idx * params.token_stride + d_idx * params.dim_stride;
                if (d_idx < params.head_dim && b_r_idx < b_r_limit)
                {
                    tile_o[o_idx] = __float2bfloat16(__uint_as_float(tc_O[tc_CD_idx][tc_m_idx][tc_n_idx]) / shmem->ls[b_r_idx]);
                }
            }
        }
    }
    if (b_r_idx < b_r_limit)
    {
        tile_L[b_r_idx] = m + __float2bfloat16(log(l));
    }
}

void launch_flash_attention(
    const num *q, const num *k, const num *v, num *o, num *L, attention_params params)
{
    constexpr int splits = 2;
    constexpr int B_r = 64;
    constexpr int B_c_split_size = 64;
    int T_r = ceil_div(params.seqlen, B_r);
    dim3 blocks = dim3(params.batch_size, params.nheads, T_r);
    dim3 threads = dim3(B_r, splits);
    constexpr int d_split_size = 64;
    assert(d_split_size * splits >= params.head_dim);
    typedef shmem_t<B_r, B_c_split_size, d_split_size, splits> shmem_t_instance;
    CUDA_CHECK(cudaFuncSetAttribute(
        flash_attention<B_r, B_c_split_size, d_split_size, splits>,
        cudaFuncAttributeMaxDynamicSharedMemorySize,
        sizeof(shmem_t_instance)));
    flash_attention<B_r, B_c_split_size, d_split_size, splits>
        <<<blocks, threads, sizeof(shmem_t_instance)>>>(q, k, v, o, L, params);
}